# -*- coding: utf-8 -*-
"""Proyek Analisis Sentimen_Bimo Birra.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pl92mxfBrlfjHH1abpjXhmks_dRZP9PV

#Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import accuracy_score

import re
import csv
import string
import requests
from io import StringIO

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

!pip install sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

from wordcloud import WordCloud

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

"""#Data Loading"""

df = pd.read_csv('ulasan.csv')

df.head()

df.info()

#Remove Missing Values
clean_df = df.dropna()

#Remove Duplicated Values
clean_df = df.drop_duplicates()

clean_df.info()

"""#Preprocessing Text"""

def cleaningText(text):
    text = str(text)
    text = re.sub(r'@[A-Za-z0-9]+', '', text)
    text = re.sub(r'#[A-Za-z0-9]+', '', text)
    text = re.sub(r'RT[\s]', '', text)
    text = re.sub(r"http\S+", '', text)
    text = re.sub(r'\d+\.\d+', '', text)
    text = re.sub(r'[0-9]+', '', text)
    text = re.sub(r'[^\w\s]', '', text)

    text = text.replace('\n', ' ')
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.strip(' ')
    return text

def casefoldingText(text):
    text = text.lower()
    return text

def tokenizingText(text):
    text = word_tokenize(text)
    return text

def filteringText(text):
    listStopwords = set(stopwords.words('indonesian'))
    listStopwords1 = set(stopwords.words('english'))
    listStopwords.update(listStopwords1)
    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',"di","ga","ya","gaa","loh","kah","woi","woii","woy", 'gk'])
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered
    return text

def toSentence(list_words):
    sentence = ' '.join(word for word in list_words)
    return sentence

slangwords = {"@": "di", "abis": "habis", "wtb": "beli", "masi": "masih", "wts": "jual", "wtt": "tukar", "bgt": "banget", "maks": "maksimal"}

def fix_slang(text):
  words = text.split()
  fixed_words = []

  for word in words:
    if word.lower() in slangwords:
      fixed_words.append(slangwords[word.lower()])
    else:
      fixed_words.append(word)

  fixed_text = ' '.join(fixed_words)

  return fixed_text

clean_df['text_clean'] = clean_df['content'].apply(cleaningText)

clean_df['text_casefolding'] = clean_df['text_clean'].apply(casefoldingText)

clean_df['text_slangwords'] = clean_df['text_casefolding'].apply(fix_slang)

clean_df['text_tokenize'] = clean_df['text_slangwords'].apply(tokenizingText)

clean_df['text_stopwords'] = clean_df['text_tokenize'].apply(filteringText)

clean_df['text_akhir'] = clean_df['text_stopwords'].apply(toSentence)

clean_df

"""#Labeling"""

lexicon_positive = dict()

response = requests.get('https://raw.githubusercontent.com/fajri91/InSet/refs/heads/master/positive.tsv')

if response.status_code == 200:
  reader = csv.reader(StringIO(response.text), delimiter='\t')

  next(reader)

  for row in reader:
    lexicon_positive[row[0]] = int(row[1])

else:
  print('Failed to fetch data')

lexicon_negative = dict()

response = requests.get('https://raw.githubusercontent.com/fajri91/InSet/refs/heads/master/negative.tsv')

if response.status_code == 200:
  reader = csv.reader(StringIO(response.text), delimiter='\t')

  next(reader)

  for row in reader:
    lexicon_negative[row[0]] = int(row[1])

else:
  print('Failed to fetch data')

def sentiment_analysis(text):

  score = 0

  for word in text:

    if word in lexicon_positive:
      score += lexicon_positive[word]

  for word in text:

    if word in lexicon_negative:
      score += lexicon_negative[word]

  polarity = ''

  if score > 0:
    polarity = 'positive'
  elif score < 0:
    polarity = 'negative'
  else:
    polarity = 'neutral'

  return score, polarity

results = clean_df['text_stopwords'].apply(sentiment_analysis)

results = list(zip(*results))

clean_df['polarity_score'] = results[0]
clean_df['polarity'] = results[1]

print(clean_df['polarity'].value_counts())

plt.pie(clean_df['polarity'].value_counts(), labels=clean_df['polarity'].value_counts().index, autopct='%1.1f%%')
plt.title('Sentiment Polarity for Roblox Review on Google Play')
plt.show()

text = ' '.join(clean_df['text_akhir'].astype(str).tolist())

all = WordCloud().generate(text)
plt.imshow(all, interpolation='bilinear')
plt.axis("off")
plt.show()

positive_reviews = clean_df[clean_df['polarity'] == 'positive']
positive_text = ' '.join(positive_reviews['text_akhir'].astype(str).tolist())

wordcloud_positive = WordCloud().generate(positive_text)

plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis("off")
plt.show()

negative_reviews = clean_df[clean_df['polarity'] == 'negative']
negative_text = ' '.join(negative_reviews['text_akhir'].astype(str).tolist())

wordcloud_negative = WordCloud().generate(negative_text)

plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis("off")
plt.show()

neutral_reviews = clean_df[clean_df['polarity'] == 'neutral']
neutral_text = ' '.join(neutral_reviews['text_akhir'].astype(str).tolist())

wordcloud_neutral = WordCloud().generate(neutral_text)

plt.imshow(wordcloud_neutral, interpolation='bilinear')
plt.axis("off")
plt.show()

# Split Data

X = clean_df['text_akhir']
y = clean_df['polarity']

"""#Skema 1"""

# Tokenizer
tokenizer = Tokenizer(num_words=5000, oov_token='<oov>')
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)
X = pad_sequences(sequences, maxlen=100)

y = pd.get_dummies(clean_df['polarity'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.92):
      print("\nAkurasi telah mencapai >92%!")
      self.model.stop_training = True
callbacks = myCallback()

# Model LSTM dan Embedding dengan pembagian data 80-20
model1 = Sequential([
    Embedding(input_dim=5000, output_dim=50),
    LSTM(128),
    Dense(3, activation='softmax'),
])

model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model1.fit(X_train, y_train, epochs=10, batch_size=64, callbacks=[callbacks])

train_loss, train_accuracy = model1.evaluate(X_train, y_train, batch_size=64)
test_loss, test_accuracy = model1.evaluate(X_test, y_test, batch_size=64)

print(f'Train Accuracy: {train_accuracy:.4f}')
print(f'Test Accuracy: {test_accuracy:.4f}')

"""#Skema 2"""

# Menggunakan Random Forest dan TF-IDF

vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = vectorizer.fit_transform(clean_df['text_akhir'])

# Label Encoder

encoder = LabelEncoder()
y = encoder.fit_transform(clean_df['polarity'])

# Pembagian Data 80/20

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Model Random Forest

model2 = RandomForestClassifier(n_estimators=100, random_state=42)

model2.fit(X_train.toarray(), y_train)

y_train_pred = model2.predict(X_train)
y_test_pred = model2.predict(X_test)

acc_train = accuracy_score(y_train, y_train_pred)
acc_test = accuracy_score(y_test, y_test_pred)

print(f'Train Accuracy: {acc_train:.4f}')
print(f'Test Accuracy: {acc_test:.4f}')

"""#Skema 3"""

# Menggunakan Naive Bayes dan Bag of Words

bow = CountVectorizer(max_features=5000)
X_bow = bow.fit_transform(clean_df['text_akhir'])

# Label Encoder
le = LabelEncoder()
y = le.fit_transform(clean_df['polarity'])

#Pembagian Data 70/30

X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.3, random_state=42)

# Model Naive Bayes

model3 = BernoulliNB()

model3.fit(X_train, y_train)

y_train_pred = model3.predict(X_train)
y_test_pred = model3.predict(X_test)

acc_train = accuracy_score(y_train, y_train_pred)
acc_test = accuracy_score(y_test, y_test_pred)

print(f'Train Accuracy: {acc_train:.4f}')
print(f'Test Accuracy: {acc_test:.4f}')

"""# Kesimpulan

Dari data di atas bisa kita lihat kalau model yang bekerja paling baik adalah model LSTM kemudian RandomForestClassifier, dan NaiveBayes di peringkat terakhir

#Inference
"""

# Input Kalimat
kalimat = [
    'game ini sangat bagus',
    'tolong update gamenya',
    'gamenya jelek',
]

tes_feature = tokenizer.texts_to_sequences(kalimat)
pad = pad_sequences(tes_feature, maxlen=100)

# Predict
predictions = model1.predict(pad)

label = ['negative', 'positive', 'neutral']
predicted_labels = np.argmax(predictions, axis=1)

for pred in predicted_labels:
  print(label[pred])

# Export Model

import joblib

model1.save('lstm_model.h5')
joblib.dump(tokenizer, 'tokenizer.pkl')